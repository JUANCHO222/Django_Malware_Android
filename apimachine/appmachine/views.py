from django.shortcuts import render
from django.http import HttpResponse
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import confusion_matrix, recall_score, f1_score, precision_score
from pandas import DataFrame
from matplotlib.colors import ListedColormap
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier
from graphviz import Source
from sklearn.tree import export_graphviz
from sklearn.ensemble import RandomForestClassifier
from django.http import HttpResponse
from matplotlib.colors import ListedColormap
import matplotlib.pyplot as plt
from django.conf import settings
import io
import os
from graphviz import Source
from sklearn.tree import export_graphviz
import os
from django.conf import settings


# Importa la configuración de Django
from django.conf import settings


# Create your views here.
def index(request):
    return render(request, 'index.html')


# _____________Modelado de Machine Learning ______________


def train_val_test_split(df, rstate=42, shuffle=True, stratify=None):
    strat = df[stratify] if stratify else None
    train_set, test_set = train_test_split(
        df, test_size=0.4, random_state=rstate, shuffle=shuffle, stratify=strat)
    strat = test_set[stratify] if stratify else None
    val_set, test_set = train_test_split(
        test_set, test_size=0.5, random_state=rstate, shuffle=shuffle, stratify=strat)
    return (train_set, val_set, test_set)

def remove_labels(df, label_name):
    X = df.drop(label_name, axis=1)
    y = df[label_name].copy()
    return (X, y)

def evaluate_result(y_pred, y, y_prep_pred, y_prep, metric):
    print(metric.__name__, "WITHOUT preparation:", metric(y_pred, y, average='weighted'))
    print(metric.__name__, "WITH preparation:", metric(y_prep_pred, y_prep, average='weighted'))


#_______________________Lectura de datos__________________


df = pd.read_csv('TotalFeatures-ISCXFlowMeter.csv', nrows=70000)
#df = pd.read_csv('TotalFeatures-ISCXFlowMeter.csv')

def mostarDatos(request):
    d = df.head(10)
    num_rows, num_cols = df.shape
    data_types = df.dtypes
    descriptive_stats = df.describe()
    
    info = {
        'num_rows': num_rows,
        'num_cols': num_cols,
        'data_types': data_types,
        'descriptive_stats': descriptive_stats
    }
    
    return render(request, 'informacion.html', {'h': d, 'infoZZZ': info})



# ______________________Correlaciones______________________


X = df.copy()
X['calss'] = X['calss'].factorize()[0]
corr_matrix = X.corr()
corr_matrix["calss"].sort_values(ascending=False)
X.corr()
# Se puede llegar a valorar quedarnos con aquellas que tienen mayor correlación
corr_matrix[corr_matrix["calss"] > 0.05]

# Division del conjunto de datos

train_set, val_set, test_set = train_val_test_split(X)

X_train, y_train = remove_labels(train_set, 'calss')
X_val, y_val = remove_labels(val_set, 'calss')
X_test, y_test = remove_labels(test_set, 'calss')

# Escalado del connjunto de datos

scaler = RobustScaler()
X_train_scaled = scaler.fit_transform(X_train)

scaler = RobustScaler()
X_test_scaled = scaler.fit_transform(X_test)

scaler = RobustScaler()
X_val_scaled = scaler.fit_transform(X_val)

# Transformación a un DataFrame de Pandas
X_train_scaled = DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)


# ______________________Árbol de decisión______________________


def mostarArbol(request):
    MAX_DEPTH = 20

    # Modelo entrenado con el conjunto de datos sin escalar
    clf_tree = DecisionTreeClassifier(max_depth=MAX_DEPTH, random_state=42)
    clf_tree.fit(X_train, y_train)

    # Modelo entrenado con el conjunto de datos escalado
    clf_tree_scaled = DecisionTreeClassifier(max_depth=MAX_DEPTH, random_state=42)
    clf_tree_scaled.fit(X_train_scaled, y_train)

    # Predecimos con el conjunto de datos de entrenamiento
    y_train_pred = clf_tree.predict(X_train)
    y_train_prep_pred = clf_tree_scaled.predict(X_train_scaled)

    # Evaluamos los resultados
    train_metric_without_prep = f1_score(y_train_pred, y_train, average='weighted')
    train_metric_with_prep = f1_score(y_train_prep_pred, y_train, average='weighted')

    # Predecimos con el conjunto de datos de validación
    y_pred = clf_tree.predict(X_val)
    y_prep_pred = clf_tree_scaled.predict(X_val_scaled)

    # Evaluamos los resultados
    val_metric_without_prep = f1_score(y_pred, y_val, average='weighted')
    val_metric_with_prep = f1_score(y_prep_pred, y_val, average='weighted')


    context = {
        'train_metric_without_prep': train_metric_without_prep,
        'train_metric_with_prep': train_metric_with_prep,
        'val_metric_without_prep': val_metric_without_prep,
        'val_metric_with_prep': val_metric_with_prep
    }

    return render(request, 'arboles.html', context)


# ______________________Grafico___________________________

# Reducimos el número de atributos del conjunto de datos para visualizarlo mejor
X_train_reduced = X_train[['min_flowpktl', 'flow_fin']]

# Representamos gráficamente el límite de decisión construido
# Generamos un modelo con el conjunto de datos reducido
clf_tree_reduced = DecisionTreeClassifier(max_depth=2, random_state=42)
clf_tree_reduced.fit(X_train_reduced, y_train)

def plot_decision_boundary(clf, X, y, plot_training=True, resolution=1000):
    mins = X.min(axis=0) - 1
    maxs = X.max(axis=0) + 1
    x1, x2 = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),
                        np.linspace(mins[1], maxs[1], resolution))
    X_new = np.c_[x1.ravel(), x2.ravel()]
    y_pred = clf.predict(X_new).reshape(x1.shape)
    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])
    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)
    custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])
    plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)
    if plot_training:
        plt.plot(X[:, 0][y==0], X[:, 1][y==0], "yo", label="normal")
        plt.plot(X[:, 0][y==1], X[:, 1][y==1], "bs", label="adware")
        plt.plot(X[:, 0][y==2], X[:, 1][y==2], "g^", label="malware")
        plt.axis([mins[0], maxs[0], mins[1], maxs[1]])               
    plt.xlabel('min_flowpktl', fontsize=14)
    plt.ylabel('flow_fin', fontsize=14, rotation=90)

    # Guardar la figura en formato PNG en la carpeta static
    plt.savefig(os.path.join(settings.BASE_DIR, 'static', 'images', 'decision_boundary.png'))

plt.figure(figsize=(12, 6))
plot_decision_boundary(clf_tree_reduced, X_train_reduced.values, y_train)
plt.close()  # Cerrar la figura después de guardarla

# Definir la ruta para el archivo DOT dentro del directorio estático
dot_file_path = os.path.join(settings.BASE_DIR, 'static', 'images', 'android_malware.dot')

# Guardar el árbol en formato DOT en el directorio estático
export_graphviz(
    clf_tree_reduced,
    out_file=dot_file_path,
    feature_names=X_train_reduced.columns,
    class_names=["benign", "adware", "malware"],
    rounded=True,
    filled=True
)

# Convertir el archivo DOT a PNG
output_file_path = os.path.join(settings.BASE_DIR, 'static', 'images', 'android_malware.png')
Source.from_file(dot_file_path).render(output_file_path, format="png", cleanup=True)

# ______________________Random Forest______________________

def mostarRandomForest(request):

    # Modelo entrenado con el conjunto de datos sin escalar
    clf_rnd = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    clf_rnd.fit(X_train, y_train)
    # Predecimos con el conjunto de datos de validación
    y_pred = clf_rnd.predict(X_val)
    clf_forest=  f1_score(y_pred, y_val, average='weighted')
    context={
        'clf_forest': clf_forest
    }
    return render(request, 'bosques.html',context)


# ______________________Caracteristicas ______________________

def mostrarCaracteristicas(request):
    clf_rnd = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    clf_rnd.fit(X_train, y_train)
    clf_rndinfo = clf_rnd.feature_importances_

    # Extraemos las características más importantes para la clasificación de datos
    feature_importances = {name: score for name, score in zip(list(df), clf_rnd.feature_importances_)}
    feature_importances_sorted = pd.Series(feature_importances).sort_values(ascending=False)
    moreImportant = feature_importances_sorted.head(20)

    # Extraemos las 10 características más relevantes para el algoritmo
    columns = list(feature_importances_sorted.head(10).index)
    X_train_reduced = X_train[columns].copy()
    X_val_reduced = X_val[columns].copy()
    reduxCharacters = X_train_reduced.head(10)

    clf_rnd_reduced = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)
    clf_rnd_reduced.fit(X_train_reduced, y_train)
    y_pred = clf_rnd_reduced.predict(X_val_reduced)
    prediccionData = f1_score(y_pred, y_val, average='weighted')

    context={
        'clf_rndinfo': clf_rndinfo,
        'moreImportant': moreImportant,
        'columns': columns,
        'reduxCharacters': reduxCharacters,
        'prediccionData': prediccionData
    }
    return render(request, 'caracteristicas.html', context)

# ______________________PCA______________________

def mostarPCA(request):

    # Separamos las variables de entrada (X) de la etiqueta (y)
    # Transformamos y a valor numérico
    X_df, y_df = remove_labels(df, 'calss')
    y_df = y_df.factorize()[0]

    # Reducimos el conjunto de datos a 2 dimensiones utilizando el algoritmo PCA
    from sklearn.decomposition import PCA

    pca = PCA(n_components=2)
    df_reduced = pca.fit_transform(X_df)
    df_reduced = pd.DataFrame(df_reduced, columns=["c1", "c2"])
    pca_reduced = df_reduced.head(10)
    
    import matplotlib.pyplot as plt

    plt.figure(figsize=(12, 6))
    plt.plot(df_reduced["c1"][y_df==0], df_reduced["c2"][y_df==0], "yo", label="normal")
    plt.plot(df_reduced["c1"][y_df==1], df_reduced["c2"][y_df==1], "bs", label="adware")
    plt.plot(df_reduced["c1"][y_df==2], df_reduced["c2"][y_df==2], "g^", label="malware")
    plt.xlabel("c1", fontsize=15)
    plt.ylabel("c2", fontsize=15, rotation=0)
    plt.savefig(os.path.join(settings.BASE_DIR, 'static', 'images', 'data_conjunt.png'))


    # Generamos un modelo con el conjunto de datos reducido
    from sklearn.tree import DecisionTreeClassifier

    clf_tree_reduced = DecisionTreeClassifier(max_depth=3, random_state=42)
    clf_tree_reduced.fit(df_reduced, y_df)

    # Reducimos el conjunto de datos manteniendo el 99,9% de varianza
    from sklearn.decomposition import PCA

    pca = PCA(n_components=0.999)
    df_reduced = pca.fit_transform(X_df)

    componentes = pca.n_components_
    proporcionVarian = pca.explained_variance_ratio_


    #df_reduced = pd.DataFrame(df_reduced, columns=["c1", "c2", "c3", "c4", "c5", "c6"])
    df_reduced = pd.DataFrame(df_reduced, columns=["c1", "c2", "c3", "c4"])
    df_reduced["Class"] = y_df
    transformacion = df_reduced.head(10)

    # Dividimos el conjunto de datos
    train_set, val_set, test_set = train_val_test_split(df_reduced)
    X_train, y_train = remove_labels(train_set, 'Class')
    X_val, y_val = remove_labels(val_set, 'Class')
    X_test, y_test = remove_labels(test_set, 'Class')

    from sklearn.ensemble import RandomForestClassifier

    clf_rnd = RandomForestClassifier(n_estimators=200, max_depth=30, random_state=42, n_jobs=-1)
    bosques = clf_rnd.fit(X_train, y_train)

    # Predecimos con el conjunto de datos de validación
    y_val_pred = clf_rnd.predict(X_val)

    # F1 score conjunto de datos de validación
    cdv = f1_score(y_val_pred, y_val, average='weighted')
    # Predecimos con el conjunto de datos de pruebas
    y_test_pred = clf_rnd.predict(X_test)

    # F1 score conjunto de datos de pruebas
    cdp = f1_score(y_test_pred, y_test, average='weighted')
    context = {
        'pca_reduced':pca_reduced,
        'componentes':componentes,
        'proporcionVarian':proporcionVarian,
        'transformacion':transformacion,
        'bosques':bosques,
        'cdv':cdv,
        'cdp':cdp
    }
    return render(request, 'extraccion.html', context)

# ______________________Seleccion del modelo______________________ 

def mostarSeleccion(request):

    long_data = len(df)
    long_column_data = len(df.columns)
    tabla = df["calss"].value_counts()

    # Dividimos el conjunto de datos
    train_set, val_set, test_set = train_val_test_split(df) 

    X_train, y_train = remove_labels(train_set, 'calss')
    X_val, y_val = remove_labels(val_set, 'calss')
    X_test, y_test = remove_labels(test_set, 'calss')

    from sklearn.ensemble import RandomForestClassifier

    # Modelo entrenado con el conjunto de datos sin escalar
    clf_rnd = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    clf_rnd.fit(X_train, y_train)

    # Predecimos con el conjunto de datos de validación
    y_pred = clf_rnd.predict(X_val)

    valData =  f1_score(y_pred, y_val, average='weighted')


    from sklearn.model_selection import RandomizedSearchCV
    from scipy.stats import randint

    param_distribs = {
            'n_estimators': randint(low=1, high=200),
            'max_depth': randint(low=8, high=50),
        }

    rnd_clf = RandomForestClassifier(n_jobs=-1)

    # train across 2 folds, that's a total of 5*2=10 rounds of training
    rnd_search = RandomizedSearchCV(rnd_clf, param_distributions=param_distribs,
                                    n_iter=5, cv=2, scoring='f1_weighted')

    valoresRamdom = rnd_search.fit(X_train, y_train)

    nose1 = rnd_search.best_params_

    nose2 = rnd_search.best_estimator_

    cvres = rnd_search.cv_results_
    html_lines = []  # Lista para almacenar las líneas HTML
    for mean_score, params in zip(cvres["mean_test_score"], cvres["params"]):
        html_line = f"<p>F1 score: {mean_score} - Parámetros: {params}</p>"
        html_lines.append(html_line)

    nose3 = rnd_search.best_estimator_.get_params()
    clf_rnd = rnd_search.best_estimator_
    y_train_pred = clf_rnd.predict(X_train)

    cde = f1_score(y_train_pred, y_train, average='weighted')
    y_val_pred = clf_rnd.predict(X_val)
    cdv = f1_score(y_val_pred, y_val, average='weighted')


    context = {
        'long_data':long_data,
        'long_column_data':long_column_data,
        'tabla':tabla,
        'valData':valData,
        'valoresRamdom':valoresRamdom,
        'nose1':nose1,
        'nose2':nose2,
        'html_lines': html_lines,
        'nose3':nose3, 
        'cde':cde,
        'cdv':cdv,
    }

    return render(request, 'modelo.html', context)